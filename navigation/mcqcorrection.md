---
layout: post 
title: MCQ
search_exclude: true
permalink: /blogs/mcq
comments: true
---
# üß† Homework Hacks: Responsible AI & Innovation
## üí° Homework Hack #1: Rethinking AI for New Uses
**Task**: AI is often used in new ways that weren‚Äôt planned. Your job is to think of a new way to use an AI invention.  
**Example**:  
```markdown
**AI Innovation**: Facial Recognition  
**Original Use**: Security and identity verification (e.g., unlocking phones, airport check-ins)  
**New Use Case**: Used in wildlife conservation to track and monitor endangered animal populations in the wild.  

**Impact**:
- **Benefits**:
  - Helps prevent poaching by identifying animals and alerting rangers.
  - Gathers data to support biodiversity research.
- **Risks**:
  - Could be misused to track people or violate privacy if not secured properly.
  - Expensive to implement in remote areas without proper infrastructure.
  

# üõ†Ô∏è Homework Hack #2: Ethical AI Coding Challenge
## Task: AI can cause problems if not designed well. Your job is to find a problem AI has caused and think of ways to fix it.

### ‚úÖ 

**Problem**: AI in image recognition misidentifies people based on race.

**Risk**: Can lead to wrongful arrests or discrimination, especially in law enforcement systems.

**Solutions**:
1. Train models with diverse and representative datasets to reduce bias.
2. Require human oversight and explainable AI (XAI) techniques for every critical decision.

**Reflection**:
Ethical AI development is important because biased systems can hurt real people. By including human checks and diverse data, we make technology more fair and accountable.
---
# üïµÔ∏è Homework Hack #3: AI & Unintended Consequences Research Task
## Task: Many AI systems have unexpected effects on society. Your goal is to find and analyze a real-world AI example that led to unintended consequences.

### ‚úÖ Example

**AI Example**: Amazon‚Äôs AI recruitment tool

**What Happened**:  
Amazon built an AI to screen job applicants. It started downgrading resumes that included the word "women" or came from all-women‚Äôs colleges, because it learned from historical hiring data that favored men.

**Response**:  
Amazon scrapped the tool after discovering the bias and never used it in production.

**Prevention**:  
Developers should have:
- Audited the training data for bias before training the model.
- Included fairness constraints and bias testing tools in the development pipeline.
